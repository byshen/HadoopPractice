import java.io.IOException;  
import java.util.HashMap;  
  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.hbase.HBaseConfiguration;  
import org.apache.hadoop.hbase.client.Put;  
import org.apache.hadoop.hbase.client.Result;  
import org.apache.hadoop.hbase.client.Scan;  
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;  
import org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat;  
import org.apache.hadoop.hbase.mapreduce.TableInputFormat;  
import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
import org.apache.hadoop.hbase.protobuf.generated.ClientProtos;
import org.apache.hadoop.hbase.util.Base64;  
import org.apache.hadoop.hbase.util.Bytes;  
import org.apache.hadoop.io.Writable;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.Mapper;  
import org.apache.hadoop.util.GenericOptionsParser;  

public class IndexBuilder {
	// 索引的列族为info ，列为name
	public static final byte[] column = Bytes.toBytes(“info”);
	public static final byte[] qualifier = Bytes.toBytes(“name”);

	public static class Map extends Mapper<ImmutableBytesWritable, Result, ImmutableBytesWritable, Writable> {
		private byte[] family;
		// hashmap存储了列与表对对应关系，其中byte［］用于获取列的值作为索引表的键值，ImmutableBytesWritable作为表的名称
		private HashMap<byte[], ImmutableBytesWritable> indexes;

		protected void map(ImmutableBytesWritable rowKey, Result result,Context context) throws IOException, InterruptedException {
			for (java.util.Map.Entry<byte[], ImmutableBytesWritable> index : indexes.entrySet()) {
				byte[] qualifier = index.getKey();// 获得列名
				ImmutableBytesWritable table = index.getValue();// 索引表名 table
				byte[] value = result.getValue(family, qualifier);// 根据列族＋列名获得元素的值
				// 以列值作为行键，在列“info：row”中插入行键
				Put put = new Put(value);
				put.add(column, qualifier, rowKey.get());
				// 在table表上执行put操作
				//在hbase的较高版本中，context.write类的第二个参数需要使能writable，否则无法写入				
				context.write(table, (Writable) put);
			}
		}


		protected void setup(Context context) throws IOException,InterruptedException {
			Configuration configuration = context.getConfiguration();
			String table = configuration.get("index.tablename");
			String[] fields = configuration.getStrings("index.fields");
			String family = configuration.get("index.familyname");
			family = Bytes.toBytes(family);

			// 初始化indexes
			indexes = new HashMap<byte[], ImmutableBytesWritable>();
			for (String field : fields) {
				indexes.put(Bytes.toBytes(field), new ImmutableBytesWritable(
						Bytes.toBytes(table + "-" + field)));
			}
		}
	}

	public static Job configureJob(Configuration conf, String[] args)throws IOException {
		String table = args[0];
		String columnFamily = args[1];
		System.out.println(table);
		// 通过Configuration.set()方法传递参数
		conf.set(TableInputFormat.SCAN,convertScanToString(new Scan()));
		conf.set(TableInputFormat.INPUT_TABLE, table);
		conf.set("index.tablename", table);
		conf.set("index.familyname", columnFamily);
		String[] fields = new String[args.length - 2];
		for (int i = 0; i < fields.length; i++) {
			fields[i] = args[i + 2];
		}
		conf.setStrings("index.fields", fields);
		conf.set("index.familyname", "attributes");

		// 运行参数配置
		Job job = new Job(conf, table);
		job.setJarByClass(IndexBuilder.class);
		job.setMapperClass(Map.class);
		job.setNumReduceTasks(0);
		job.setInputFormatClass(TableInputFormat.class);
		job.setOutputFormatClass(MultiTableOutputFormat.class);
		return job;
	}

	public static void main(String[] args) throws Exception {
		Configuration conf = HBaseConfiguration.create();
		String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
		if (otherArgs.length < 3)System.exit(-1);
		Job job = configureJob(conf, otherArgs);
		System.exit(job.waitForCompletion(true) ? 0 : 1);
	}
	static String convertScanToString(Scan scan) throws IOException {
	    ClientProtos.Scan proto = ProtobufUtil.toScan(scan);
	    return Base64.encodeBytes(proto.toByteArray());
	}
}